{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd5d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable TensorFlow to avoid tf_keras import errors when using PyTorch\n",
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['USE_TF'] = '0'\n",
    "\n",
    "# Suppress warnings before importing transformers\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np    # linear algebra\n",
    "import pandas as pd   # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import transformers (tf_keras is now installed to prevent import errors)\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification, BertModel, AdamWeightDecay, get_linear_schedule_with_warmup,\n",
    "                          TrainingArguments, Trainer)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f20eddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes after filtering: 3\n",
      "Top classes: ['no' 'three' 'yes']\n",
      "\n",
      "Train size: 752\n",
      "Val size: 188\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "df_08 = pd.read_csv('S08_question_answer_pairs.txt', sep='\\t')\n",
    "df_09 = pd.read_csv('S09_question_answer_pairs.txt', sep='\\t')\n",
    "df_10 = pd.read_csv('S10_question_answer_pairs.txt', sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "# 1) Basic cleaning\n",
    "df = pd.concat([df_08, df_09, df_10])\n",
    "df = df[['Question', 'Answer']].dropna()\n",
    "df = df[(df['Question'].str.len() > 0) & (df['Answer'].str.len() > 0)]\n",
    "df = df.drop_duplicates(subset=['Question'])\n",
    "\n",
    "# 2) Normalize answers to avoid label explosion\n",
    "#    - lowercase, strip\n",
    "#    - remove trailing punctuation (. ! ?)\n",
    "df['Answer'] = df['Answer'].astype(str).str.lower().str.strip()\n",
    "df['Answer'] = df['Answer'].str.replace(r'[\\\"“”]', '', regex=True)\n",
    "df['Answer'] = df['Answer'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "df['Answer'] = df['Answer'].str.replace(r'[\\.!\\?]+$', '', regex=True).str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# 3) Filter rare answers AFTER normalization\n",
    "min_count = 5  # raise/lower as needed; higher -> fewer classes\n",
    "vc = df['Answer'].value_counts()\n",
    "keep = vc[vc >= min_count].index\n",
    "if len(keep) == 0:\n",
    "    keep = vc.index  # fallback if threshold too high\n",
    "    min_count = 1\n",
    "\n",
    "df = df[df['Answer'].isin(keep)]\n",
    "\n",
    "# 4) Keep top-K frequent answers; map others to \"other\"\n",
    "top_k = 100  # adjust (e.g., 50 or 100)\n",
    "vc = df['Answer'].value_counts()\n",
    "keep = vc.index[:top_k]\n",
    "df['Answer'] = df['Answer'].where(df['Answer'].isin(keep), other='other')\n",
    "\n",
    "\n",
    "# 5) Re-fit LabelEncoder AFTER filtering\n",
    "le = LabelEncoder()\n",
    "df['Encoded_Answer'] = le.fit_transform(df['Answer'])\n",
    "num_labels = len(le.classes_)\n",
    "print(f\"\\nClasses after filtering: {num_labels}\")\n",
    "print(f\"Top classes: {le.classes_[:10]}\")\n",
    "\n",
    "# 5) Stratified split (only if each class has >=2 samples)\n",
    "min_class_count = df['Encoded_Answer'].value_counts().min()\n",
    "use_stratify = (df['Encoded_Answer'].nunique() > 1) and (min_class_count > 1)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['Question'].tolist(),\n",
    "    df['Encoded_Answer'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['Encoded_Answer'] if use_stratify else None\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {len(train_texts)}\")\n",
    "print(f\"Val size: {len(val_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b965a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x17e73e640>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: aabdbc44-e9c4-4ad3-a75e-cfc2658612dd)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x17e73ec40>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: fa2e5600-ab73-4000-9aba-1891c5413308)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x31ce4b6a0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 9278ff38-b9e4-4d63-8de7-d69f1080cae1)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x31ce4b370>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: bad67d13-7411-4b3a-9c11-1c034836cff4)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x31ce4b100>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 55f52643-9a2a-44a3-ae28-a311fd10ea9e)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x31ce47be0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 4428a637-4fb7-43d0-8ebd-9707e0636b3e)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Creating Train and Validation Datasets\n",
      "======================================================================\n",
      "\n",
      "Train dataset size: 752\n",
      "Validation dataset size: 188\n",
      "Number of classes: 3\n",
      "\n",
      "======================================================================\n",
      "Testing Dataset\n",
      "======================================================================\n",
      "\n",
      "Sample from training set:\n",
      "  Input IDs shape: torch.Size([256])\n",
      "  Attention mask shape: torch.Size([256])\n",
      "  Token type IDs shape: torch.Size([256])\n",
      "  Label: 2 (Answer: 'yes')\n",
      "  Original question: Do sea otters live along the Pacific coast?...\n",
      "\n",
      "Sample from validation set:\n",
      "  Label: 2 (Answer: 'yes')\n",
      "  Original question: Is a string first tuned to a standard pitch?...\n"
     ]
    }
   ],
   "source": [
    "# 3. Create Dataset Class for QA Classification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 256  # Reduced from 512 for efficiency\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for Question-Answer classification\n",
    "    Takes questions and labels (encoded answers) for sequence classification\n",
    "    \"\"\"\n",
    "    def __init__(self, questions, labels, tokenizer, max_length=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            questions: List of question strings\n",
    "            labels: List of integer labels (encoded answers)\n",
    "            tokenizer: BERT tokenizer\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.questions = questions\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns tokenized question with label\n",
    "        Format compatible with Trainer API\n",
    "        \"\"\"\n",
    "        question = str(self.questions[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        # Tokenize question only (for classification task)\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'token_type_ids': encoding['token_type_ids'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets using the train/val split from Cell 1\n",
    "print(\"=\"*70)\n",
    "print(\"Creating Train and Validation Datasets\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if train_texts and val_texts exist from Cell 1\n",
    "if 'train_texts' in globals() and 'val_texts' in globals():\n",
    "    train_dataset = QADataset(train_texts, train_labels, tokenizer, max_length=max_length)\n",
    "    val_dataset = QADataset(val_texts, val_labels, tokenizer, max_length=max_length)\n",
    "    \n",
    "    print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    print(f\"Number of classes: {len(le.classes_)}\")\n",
    "    \n",
    "    # Test the dataset\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Testing Dataset\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    sample = train_dataset[0]\n",
    "    print(f\"\\nSample from training set:\")\n",
    "    print(f\"  Input IDs shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"  Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "    print(f\"  Token type IDs shape: {sample['token_type_ids'].shape}\")\n",
    "    print(f\"  Label: {sample['labels'].item()} (Answer: '{le.inverse_transform([sample['labels'].item()])[0]}')\")\n",
    "    print(f\"  Original question: {train_texts[0][:100]}...\")\n",
    "    \n",
    "    sample_val = val_dataset[0]\n",
    "    print(f\"\\nSample from validation set:\")\n",
    "    print(f\"  Label: {sample_val['labels'].item()} (Answer: '{le.inverse_transform([sample_val['labels'].item()])[0]}')\")\n",
    "    print(f\"  Original question: {val_texts[0][:100]}...\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: train_texts and val_texts not found!\")\n",
    "    print(\"Please run Cell 1 first to create the train/val split.\")\n",
    "    print(\"\\nCreating a simple example instead...\")\n",
    "    \n",
    "    # Fallback: create a simple example\n",
    "    sample_questions = df['Question'].head(10).tolist()\n",
    "    sample_labels = df['Encoded_Answer'].head(10).tolist()\n",
    "    example_dataset = QADataset(sample_questions, sample_labels, tokenizer, max_length=max_length)\n",
    "    print(f\"Example dataset size: {len(example_dataset)}\")\n",
    "    print(f\"Sample: {example_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3702597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading BERT Model with LoRA for Parameter-Efficient Fine-Tuning\n",
      "======================================================================\n",
      "❌ PEFT library not installed!\n",
      "Installing PEFT...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\n",
      "Requirement already satisfied: requests in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/4paradigm/Library/Python/3.9/lib/python/site-packages (from transformers->peft) (0.22.1)\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.17.1\n",
      "✓ PEFT installed successfully!\n",
      "\n",
      "Number of answer classes: 3\n",
      "Sample classes: ['no' 'three' 'yes']\n",
      "\n",
      "======================================================================\n",
      "Step 1: Loading Base BERT Model\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base model loaded on device: cpu\n",
      "Total parameters: 109,484,547\n",
      "\n",
      "======================================================================\n",
      "Step 2: Configuring LoRA\n",
      "======================================================================\n",
      "\n",
      "LoRA Configuration:\n",
      "  Rank (r): 8\n",
      "  Alpha: 16\n",
      "  Dropout: 0.1\n",
      "  Target Modules: {'key', 'query', 'value'}\n",
      "  Bias: none\n",
      "\n",
      "======================================================================\n",
      "Step 3: Applying LoRA to Model\n",
      "======================================================================\n",
      "trainable params: 444,675 || all params: 109,929,222 || trainable%: 0.4045\n",
      "\n",
      "======================================================================\n",
      "Parameter Summary:\n",
      "======================================================================\n",
      "  Total parameters: 109,929,222\n",
      "  Trainable parameters: 444,675 (0.40%)\n",
      "  Frozen parameters: 109,484,547 (99.60%)\n",
      "\n",
      "✓ LoRA reduces trainable parameters by ~99.6%!\n",
      "  This means:\n",
      "  - Faster training\n",
      "  - Less memory usage\n",
      "  - Lower risk of overfitting\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. Load BERT Model with LoRA (Low-Rank Adaptation) for Efficient Fine-Tuning\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Loading BERT Model with LoRA for Parameter-Efficient Fine-Tuning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if PEFT is installed\n",
    "try:\n",
    "    import peft\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    print(f\"✓ PEFT version: {peft.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"❌ PEFT library not installed!\")\n",
    "    print(\"Installing PEFT...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"peft\"])\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    print(\"✓ PEFT installed successfully!\")\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "\n",
    "# Get number of classes from label encoder\n",
    "num_labels = len(le.classes_)\n",
    "print(f\"\\nNumber of answer classes: {num_labels}\")\n",
    "print(f\"Sample classes: {le.classes_[:10]}\")\n",
    "\n",
    "# Load base model (frozen, not trainable)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 1: Loading Base BERT Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "base_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "print(f\"\\nBase model loaded on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
    "\n",
    "# Configure LoRA\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 2: Configuring LoRA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    r=8,                          # LoRA rank (rank of adaptation matrices) - lower = fewer params\n",
    "    lora_alpha=16,               # LoRA alpha (scaling factor) - typically 2x rank\n",
    "    lora_dropout=0.1,            # LoRA dropout\n",
    "    target_modules=[\"query\", \"value\", \"key\"],  # Which modules to apply LoRA to\n",
    "    bias=\"none\",                 # Don't train bias parameters\n",
    ")\n",
    "\n",
    "print(f\"\\nLoRA Configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target Modules: {lora_config.target_modules}\")\n",
    "print(f\"  Bias: {lora_config.bias}\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 3: Applying LoRA to Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Get parameter counts\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"Parameter Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "print(f\"  Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\")\n",
    "print(f\"\\n✓ LoRA reduces trainable parameters by ~{(1-trainable_params/total_params)*100:.1f}%!\")\n",
    "print(\"  This means:\")\n",
    "print(\"  - Faster training\")\n",
    "print(\"  - Less memory usage\")\n",
    "print(\"  - Lower risk of overfitting\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775404f",
   "metadata": {},
   "source": [
    "## LoRA Configuration Guide\n",
    "\n",
    "### What is LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning technique that:\n",
    "- Adds small trainable matrices to the model instead of training all parameters\n",
    "- Reduces trainable parameters by 90-99%\n",
    "- Maintains similar performance to full fine-tuning\n",
    "- Uses less memory and trains faster\n",
    "\n",
    "### LoRA Parameters Explained:\n",
    "\n",
    "1. **`r` (rank)**: Rank of the low-rank matrices\n",
    "   - Lower = fewer parameters (e.g., r=4, 8, 16)\n",
    "   - Default: `r=8` (good balance)\n",
    "   - Range: 4-64 (higher rarely needed)\n",
    "\n",
    "2. **`lora_alpha`**: Scaling factor for LoRA weights\n",
    "   - Typically set to 2x rank (e.g., r=8 → alpha=16)\n",
    "   - Higher = stronger adaptation\n",
    "   - Default: `16` (for r=8)\n",
    "\n",
    "3. **`lora_dropout`**: Dropout for LoRA layers\n",
    "   - Prevents overfitting\n",
    "   - Default: `0.1` (can increase to 0.2 if overfitting)\n",
    "\n",
    "4. **`target_modules`**: Which layers to apply LoRA to\n",
    "   - `[\"query\", \"value\", \"key\"]` - attention layers (recommended)\n",
    "   - `[\"query\", \"value\"]` - fewer parameters\n",
    "   - `[\"dense\"]` - feed-forward layers (less common)\n",
    "\n",
    "### Adjusting LoRA Parameters:\n",
    "\n",
    "**For more capacity (if underfitting):**\n",
    "```python\n",
    "r=16, lora_alpha=32  # More parameters\n",
    "```\n",
    "\n",
    "**For fewer parameters (if overfitting or memory constrained):**\n",
    "```python\n",
    "r=4, lora_alpha=8  # Fewer parameters\n",
    "```\n",
    "\n",
    "**For stronger regularization:**\n",
    "```python\n",
    "lora_dropout=0.2  # Higher dropout\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abc44e",
   "metadata": {},
   "source": [
    "## LoRA Benefits for Your QA Task\n",
    "\n",
    "### Why Use LoRA?\n",
    "\n",
    "1. **Reduced Memory Usage**\n",
    "   - Only trains ~1-5% of parameters instead of 100%\n",
    "   - Can train with larger batch sizes\n",
    "   - Works on smaller GPUs/CPUs\n",
    "\n",
    "2. **Faster Training**\n",
    "   - Fewer parameters to update = faster iterations\n",
    "   - Less computation per step\n",
    "\n",
    "3. **Less Overfitting**\n",
    "   - Smaller parameter space reduces overfitting risk\n",
    "   - Especially useful for small datasets\n",
    "\n",
    "4. **Multiple Task Adaptations**\n",
    "   - Can train different LoRA adapters for different tasks\n",
    "   - Keep base model, swap adapters\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "- **Trainable Parameters**: ~1-5% of full model (instead of 100%)\n",
    "- **Performance**: Similar to full fine-tuning (often within 1-2%)\n",
    "- **Training Speed**: 2-5x faster\n",
    "- **Memory**: 50-80% reduction\n",
    "\n",
    "### LoRA vs Full Fine-Tuning:\n",
    "\n",
    "| Aspect | Full Fine-Tuning | LoRA |\n",
    "|--------|------------------|------|\n",
    "| Trainable Params | 100% (110M) | 1-5% (~1-5M) |\n",
    "| Training Speed | Baseline | 2-5x faster |\n",
    "| Memory Usage | High | Low |\n",
    "| Overfitting Risk | Higher | Lower |\n",
    "| Performance | Best | Very close (95-99%) |\n",
    "\n",
    "### When to Use LoRA:\n",
    "\n",
    "✅ **Use LoRA when:**\n",
    "- Limited GPU memory\n",
    "- Small dataset (high overfitting risk)\n",
    "- Need to train multiple tasks\n",
    "- Want faster experimentation\n",
    "\n",
    "❌ **Use Full Fine-Tuning when:**\n",
    "- Large dataset\n",
    "- Need absolute best performance\n",
    "- Have abundant compute resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2461601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Setting Up Training Configuration\n",
      "======================================================================\n",
      "✓ Accelerate version: 1.10.1\n",
      "\n",
      "Available TrainingArguments parameters: 133\n",
      "✓ Using 'eval_strategy' (modern transformers)\n",
      "\n",
      "✓ TrainingArguments created successfully!\n",
      "\n",
      "======================================================================\n",
      "Training Configuration Summary\n",
      "======================================================================\n",
      "  Learning Rate: 5e-05\n",
      "  Batch Size: 16\n",
      "  Epochs: 12\n",
      "  Weight Decay: 0.05\n",
      "  Warmup Ratio: 0.1\n",
      "  Device: mps\n",
      "  FP16: False\n",
      "  Eval Strategy: epoch\n",
      "  Save Strategy: epoch\n",
      "  Load Best Model: True\n",
      "======================================================================\n",
      "\n",
      "✓ Training configuration ready!\n"
     ]
    }
   ],
   "source": [
    "# 5. Setup Training Configuration\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Setting Up Training Configuration\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# First, check if accelerate is installed\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"✓ Accelerate version: {accelerate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"❌ Accelerate not installed!\")\n",
    "    print(\"Run: pip install --upgrade accelerate\")\n",
    "    raise\n",
    "\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def top_k_acc(logits, labels, k=3):\n",
    "    topk = np.argsort(logits, axis=1)[:, ::-1][:, :k]\n",
    "    return np.mean([labels[i] in topk[i] for i in range(len(labels))])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy and F1 score for evaluation\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(labels, preds),\n",
    "        \"micro_f1\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"weighted_f1\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"top3_acc\": top_k_acc(logits, labels, k=3),\n",
    "    }\n",
    "# Check available parameters\n",
    "import inspect\n",
    "training_args_params = inspect.signature(TrainingArguments.__init__).parameters\n",
    "print(f\"\\nAvailable TrainingArguments parameters: {len(training_args_params)}\")\n",
    "\n",
    "# Build training arguments\n",
    "# Increased learning rate to help model learn faster and break out of stuck state\n",
    "base_args = {\n",
    "    'output_dir': './bert_qa_sft_outputs',\n",
    "    'num_train_epochs': 12,  # More epochs to allow learning\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 32,\n",
    "    'learning_rate': 5e-5,  # Increased from 1e-5 to help model learn faster\n",
    "    'weight_decay': 0.05,   # Slightly reduced to allow more learning\n",
    "    'warmup_ratio': 0.1,\n",
    "    'logging_dir': './logs',\n",
    "    'logging_steps': 50,\n",
    "    'save_total_limit': 2,\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'balanced_accuracy',  # Use balanced accuracy instead of loss\n",
    "    'greater_is_better': True,  # Balanced accuracy: higher is better\n",
    "}\n",
    "\n",
    "# ✅ Set eval_strategy and save_strategy to match\n",
    "if 'eval_strategy' in training_args_params:\n",
    "    base_args['eval_strategy'] = 'epoch'\n",
    "    base_args['save_strategy'] = 'epoch'\n",
    "    print(\"✓ Using 'eval_strategy' (modern transformers)\")\n",
    "elif 'eval_strategy' in training_args_params:\n",
    "    base_args['eval_strategy'] = 'epoch'\n",
    "    base_args['save_strategy'] = 'epoch'\n",
    "    print(\"✓ Using 'eval_strategy' (legacy transformers)\")\n",
    "\n",
    "# Add optional parameters\n",
    "if 'dataloader_num_workers' in training_args_params:\n",
    "    base_args['dataloader_num_workers'] = 0\n",
    "\n",
    "if torch.cuda.is_available() and 'fp16' in training_args_params:\n",
    "    base_args['fp16'] = True\n",
    "    print(\"✓ FP16 enabled (GPU available)\")\n",
    "\n",
    "if 'report_to' in training_args_params:\n",
    "    base_args['report_to'] = 'none'\n",
    "\n",
    "# Create TrainingArguments\n",
    "try:\n",
    "    training_args = TrainingArguments(**base_args)\n",
    "    print(\"\\n✓ TrainingArguments created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error creating TrainingArguments:\")\n",
    "    print(f\"   {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Print configuration\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Configuration Summary\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Learning Rate: {training_args.learning_rate}\")\n",
    "print(f\"  Batch Size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Weight Decay: {training_args.weight_decay}\")\n",
    "print(f\"  Warmup Ratio: {training_args.warmup_ratio}\")\n",
    "print(f\"  Device: {training_args.device}\")\n",
    "print(f\"  FP16: {getattr(training_args, 'fp16', False)}\")\n",
    "print(f\"  Eval Strategy: {getattr(training_args, 'eval_strategy', getattr(training_args, 'evaluation_strategy', 'N/A'))}\")\n",
    "print(f\"  Save Strategy: {getattr(training_args, 'save_strategy', 'N/A')}\")\n",
    "print(f\"  Load Best Model: {training_args.load_best_model_at_end}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✓ Training configuration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4ded600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Analyzing Class Distribution and Creating Weighted Loss\n",
      "======================================================================\n",
      "\n",
      "Class Distribution in Training Set:\n",
      "  Class 0 ('no'): 138 samples\n",
      "  Class 1 ('three'): 4 samples\n",
      "  Class 2 ('yes'): 610 samples\n",
      "\n",
      "Calculating class weights (total samples: 752):\n",
      "  Class 0 ('no'): 138 samples → weight: 2.102\n",
      "  Class 1 ('three'): 4 samples → weight: 12.349\n",
      "  Class 2 ('yes'): 610 samples → weight: 1.000\n",
      "\n",
      "Class Weights Tensor: [2.1024484634399414, 12.349088668823242, 1.0]\n",
      "Weight ratio (max/min): 12.35x\n",
      "\n",
      "✓ Weighted Trainer class created!\n",
      "  This will help the model learn from minority classes better\n"
     ]
    }
   ],
   "source": [
    "# 5.5. Analyze Class Distribution and Create Weighted Loss Trainer\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Analyzing Class Distribution and Creating Weighted Loss\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "\n",
    "# Analyze class distribution in training set\n",
    "train_label_counts = Counter(train_labels)\n",
    "print(\"\\nClass Distribution in Training Set:\")\n",
    "for label_id, count in sorted(train_label_counts.items()):\n",
    "    class_name = le.inverse_transform([label_id])[0]\n",
    "    print(f\"  Class {label_id} ('{class_name}'): {count} samples\")\n",
    "\n",
    "# Calculate class weights (inverse frequency with stronger weighting)\n",
    "total_samples = len(train_labels)\n",
    "class_weights = []\n",
    "max_count = max(train_label_counts.values()) if train_label_counts else 1\n",
    "\n",
    "print(f\"\\nCalculating class weights (total samples: {total_samples}):\")\n",
    "for i in range(num_labels):\n",
    "    count = train_label_counts.get(i, 1)  # Avoid division by zero\n",
    "    # Stronger inverse frequency weighting (square root for less extreme weights)\n",
    "    # This gives minority classes much higher weight\n",
    "    weight = np.sqrt(max_count / count) if count > 0 else 1.0\n",
    "    class_weights.append(weight)\n",
    "    class_name = le.inverse_transform([i])[0]\n",
    "    print(f\"  Class {i} ('{class_name}'): {count} samples → weight: {weight:.3f}\")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"\\nClass Weights Tensor: {class_weights_tensor.tolist()}\")\n",
    "print(f\"Weight ratio (max/min): {max(class_weights) / min(class_weights):.2f}x\")\n",
    "\n",
    "# Create custom Trainer with weighted loss\n",
    "from transformers import Trainer\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer with class-weighted loss to handle imbalanced classes\n",
    "    \"\"\"\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights.to(self.model.device)  # Ensure weights on same device\n",
    "        print(f\"\\n✓ WeightedTrainer initialized with class weights on device: {self.class_weights.device}\")\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Override compute_loss to use weighted cross-entropy\n",
    "        Accepts **kwargs for compatibility with newer transformers versions\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Ensure class weights are on the same device as logits\n",
    "        weights = self.class_weights.to(logits.device)\n",
    "        \n",
    "        # Use weighted cross-entropy loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def log(self, logs, start_time=None):\n",
    "        \"\"\"\n",
    "        Override log to show class prediction distribution\n",
    "        Matches parent class signature for compatibility\n",
    "        \"\"\"\n",
    "        # Call parent log with correct signature\n",
    "        if start_time is not None:\n",
    "            super().log(logs, start_time)\n",
    "        else:\n",
    "            super().log(logs)\n",
    "        \n",
    "        # Every 100 steps, log prediction distribution (optional debugging)\n",
    "        if hasattr(self.state, 'global_step') and self.state.global_step % 100 == 0 and 'loss' in logs:\n",
    "            try:\n",
    "                self.model.eval()\n",
    "                sample_batch = next(iter(self.get_train_dataloader()))\n",
    "                with torch.no_grad():\n",
    "                    # Move batch to device\n",
    "                    batch_inputs = {k: v.to(self.model.device) for k, v in sample_batch.items() if k != 'labels'}\n",
    "                    outputs = self.model(**batch_inputs)\n",
    "                    preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "                    pred_dist = Counter(preds)\n",
    "                    if len(pred_dist) > 1:\n",
    "                        print(f\"  Step {self.state.global_step}: Prediction diversity - {len(pred_dist)} classes predicted\")\n",
    "                self.model.train()\n",
    "            except Exception as e:\n",
    "                pass  # Skip if there's an error (non-critical)\n",
    "\n",
    "print(\"\\n✓ Weighted Trainer class created!\")\n",
    "print(\"  This will help the model learn from minority classes better\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "456963a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Saving LoRA Model\n",
      "======================================================================\n",
      "\n",
      "✓ LoRA adapters saved to: ./bert_qa_sft_final_model\n",
      "  (Only LoRA weights are saved, base model is not included)\n",
      "\n",
      "  To use this model later:\n",
      "  ```python\n",
      "  from transformers import BertForSequenceClassification\n",
      "  from peft import PeftModel\n",
      "  \n",
      "  # Load base model\n",
      "  base_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
      "  \n",
      "  # Load LoRA adapters\n",
      "  model = PeftModel.from_pretrained(base_model, './bert_qa_sft_final_model')\n",
      "  ```\n",
      "\n",
      "======================================================================\n",
      "Optional: Merge LoRA with Base Model\n",
      "======================================================================\n",
      "Uncomment the code below to merge LoRA adapters with base model:\n",
      "  # merged_model = model.merge_and_unload()\n",
      "  # merged_model.save_pretrained(final_model_path + '_merged')\n",
      "  # tokenizer.save_pretrained(final_model_path + '_merged')\n"
     ]
    }
   ],
   "source": [
    "# 6.1. Save LoRA Model (After Training)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Saving LoRA Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_model_path = './bert_qa_sft_final_model'\n",
    "\n",
    "# Save LoRA adapters (if using LoRA)\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    if isinstance(model, PeftModel) or hasattr(model, 'peft_config'):\n",
    "        # Save LoRA adapters\n",
    "        model.save_pretrained(final_model_path)\n",
    "        tokenizer.save_pretrained(final_model_path)\n",
    "        \n",
    "        print(f\"\\n✓ LoRA adapters saved to: {final_model_path}\")\n",
    "        print(\"  (Only LoRA weights are saved, base model is not included)\")\n",
    "        print(\"\\n  To use this model later:\")\n",
    "        print(\"  ```python\")\n",
    "        print(\"  from transformers import BertForSequenceClassification\")\n",
    "        print(\"  from peft import PeftModel\")\n",
    "        print(\"  \")\n",
    "        print(\"  # Load base model\")\n",
    "        print(\"  base_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\")\n",
    "        print(\"  \")\n",
    "        print(\"  # Load LoRA adapters\")\n",
    "        print(f\"  model = PeftModel.from_pretrained(base_model, '{final_model_path}')\")\n",
    "        print(\"  ```\")\n",
    "        \n",
    "        # Optional: Merge and save full model (uncomment if you want a standalone model)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Optional: Merge LoRA with Base Model\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Uncomment the code below to merge LoRA adapters with base model:\")\n",
    "        print(\"  # merged_model = model.merge_and_unload()\")\n",
    "        print(\"  # merged_model.save_pretrained(final_model_path + '_merged')\")\n",
    "        print(\"  # tokenizer.save_pretrained(final_model_path + '_merged')\")\n",
    "    else:\n",
    "        # Regular model saving (if not using LoRA)\n",
    "        model.save_pretrained(final_model_path)\n",
    "        tokenizer.save_pretrained(final_model_path)\n",
    "        print(f\"\\n✓ Full model saved to: {final_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error saving model: {e}\")\n",
    "    print(\"Trying standard save method...\")\n",
    "    model.save_pretrained(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    print(f\"✓ Model saved to: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01b83e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Initializing Weighted Trainer and Starting Training\n",
      "======================================================================\n",
      "✓ Using WeightedTrainer with class weights\n",
      "\n",
      "✓ WeightedTrainer initialized with class weights on device: mps:0\n",
      "\n",
      "✓ Trainer initialized!\n",
      "\n",
      "Starting training...\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/564 04:32 < 09:11, 0.68 it/s, Epoch 4/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Top3 Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.957013</td>\n",
       "      <td>0.813830</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.813830</td>\n",
       "      <td>0.730299</td>\n",
       "      <td>0.299120</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.102500</td>\n",
       "      <td>0.769136</td>\n",
       "      <td>0.813830</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.813830</td>\n",
       "      <td>0.730299</td>\n",
       "      <td>0.299120</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.762343</td>\n",
       "      <td>0.813830</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.813830</td>\n",
       "      <td>0.730299</td>\n",
       "      <td>0.299120</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.775300</td>\n",
       "      <td>0.765116</td>\n",
       "      <td>0.813830</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.813830</td>\n",
       "      <td>0.730299</td>\n",
       "      <td>0.299120</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training Completed!\n",
      "======================================================================\n",
      "\n",
      "✓ Model saved to: ./bert_qa_sft_final_model\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize Weighted Trainer and Start Training\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Initializing Weighted Trainer and Starting Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use WeightedTrainer if available, otherwise fall back to regular Trainer\n",
    "try:\n",
    "    # Check if WeightedTrainer was created in previous cell\n",
    "    if 'WeightedTrainer' in globals() and 'class_weights_tensor' in globals():\n",
    "        print(\"✓ Using WeightedTrainer with class weights\")\n",
    "        trainer = WeightedTrainer(\n",
    "            class_weights=class_weights_tensor,\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,\n",
    "                early_stopping_threshold=0.0\n",
    "            )]\n",
    "        )\n",
    "    else:\n",
    "        # Fallback to regular Trainer\n",
    "        print(\"⚠ WeightedTrainer not found, using regular Trainer\")\n",
    "        from transformers import Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,\n",
    "                early_stopping_threshold=0.0\n",
    "            )]\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error creating WeightedTrainer: {e}\")\n",
    "    print(\"Falling back to regular Trainer...\")\n",
    "    from transformers import Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.0\n",
    "        )]\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Trainer initialized!\")\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Completed!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save the final model\n",
    "final_model_path = './bert_qa_sft_final_model'\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"\\n✓ Model saved to: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cdc8465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Diagnostic: Checking Model Predictions (Before Training)\n",
      "======================================================================\n",
      "Model device: cpu\n",
      "Target device: cpu\n",
      "\n",
      "Sample Predictions (Before Training):\n",
      "\n",
      "1. ✓ Question: Is a string first tuned to a standard pitch?...\n",
      "   True: yes | Predicted: yes (conf: 0.912)\n",
      "\n",
      "2. ✓ Question: Did Henri Becquerel intentionally discover radioactivity?...\n",
      "   True: no | Predicted: no (conf: 0.559)\n",
      "\n",
      "3. ✓ Question: Was Tesla rich at the time of his death?...\n",
      "   True: no | Predicted: no (conf: 0.748)\n",
      "\n",
      "4. ✓ Question: Do his works enjoy a broad popular appeal in the United Stat...\n",
      "   True: yes | Predicted: yes (conf: 0.955)\n",
      "\n",
      "5. ✓ Question: Was it derived from Latin?...\n",
      "   True: yes | Predicted: yes (conf: 0.951)\n",
      "\n",
      "📊 Prediction Diversity: 2 unique predictions out of 5 samples\n",
      "\n",
      "✓ Diagnostic complete!\n"
     ]
    }
   ],
   "source": [
    "# 6.5. Diagnostic: Check Model Predictions Before Training\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Diagnostic: Checking Model Predictions (Before Training)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ensure model is on correct device\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Check device\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Target device: {device}\")\n",
    "\n",
    "# Get a few predictions from untrained model\n",
    "sample_indices = [0, 1, 2, 5, 10]\n",
    "predictions_before = []\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for idx in sample_indices:\n",
    "            if idx >= len(val_dataset):\n",
    "                continue\n",
    "                \n",
    "            sample = val_dataset[idx]\n",
    "            \n",
    "            # Ensure all tensors are on the same device\n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "            attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "            token_type_ids = sample['token_type_ids'].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Make prediction\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            pred_id = torch.argmax(logits, dim=1).item()\n",
    "            true_id = sample['labels'].item()\n",
    "            \n",
    "            predictions_before.append({\n",
    "                'question': val_texts[idx][:60] + \"...\" if idx < len(val_texts) else \"N/A\",\n",
    "                'true_label': true_id,\n",
    "                'true_answer': le.inverse_transform([true_id])[0],\n",
    "                'pred_label': pred_id,\n",
    "                'pred_answer': le.inverse_transform([pred_id])[0],\n",
    "                'confidence': probs[0][pred_id].item()\n",
    "            })\n",
    "    \n",
    "    print(\"\\nSample Predictions (Before Training):\")\n",
    "    for i, pred in enumerate(predictions_before, 1):\n",
    "        match = \"✓\" if pred['true_label'] == pred['pred_label'] else \"✗\"\n",
    "        print(f\"\\n{i}. {match} Question: {pred['question']}\")\n",
    "        print(f\"   True: {pred['true_answer']} | Predicted: {pred['pred_answer']} (conf: {pred['confidence']:.3f})\")\n",
    "    \n",
    "    # Check if model is predicting the same class for all samples\n",
    "    if predictions_before:\n",
    "        all_preds = [p['pred_label'] for p in predictions_before]\n",
    "        unique_preds = len(set(all_preds))\n",
    "        print(f\"\\n📊 Prediction Diversity: {unique_preds} unique predictions out of {len(predictions_before)} samples\")\n",
    "        if unique_preds == 1:\n",
    "            print(\"⚠ WARNING: Model is predicting the same class for all samples!\")\n",
    "            print(\"   This suggests severe class imbalance or initialization issue.\")\n",
    "    \n",
    "    print(\"\\n✓ Diagnostic complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Error during diagnostic: {e}\")\n",
    "    print(\"This is okay - you can skip this diagnostic and proceed to training.\")\n",
    "    print(\"The weighted loss trainer should still work correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8318999b",
   "metadata": {},
   "source": [
    "# 9. Comprehensive Parameter Tuning Guide for 99.9% Accuracy\n",
    "\n",
    "## Critical Problem Identified:\n",
    "\n",
    "Your class distribution shows:\n",
    "- **Class 0 ('no')**: 138 samples (18%)\n",
    "- **Class 1 ('three')**: 4 samples (0.5%) ⚠️ **TOO FEW TO LEARN**\n",
    "- **Class 2 ('yes')**: 610 samples (81%) ⚠️ **DOMINANT CLASS**\n",
    "\n",
    "**This extreme imbalance makes 99.9% accuracy very difficult!**\n",
    "\n",
    "## Why 99.9% is Challenging:\n",
    "\n",
    "1. **Class 1 has only 4 samples** - Model cannot learn this class reliably\n",
    "2. **81% of data is 'yes'** - Model defaults to predicting 'yes'\n",
    "3. **Metrics stuck** - Model not learning, just predicting majority class\n",
    "\n",
    "## Solutions (In Order of Priority):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22428bf",
   "metadata": {},
   "source": [
    "## Realistic Expectations for 99.9% Accuracy\n",
    "\n",
    "### Current Situation:\n",
    "\n",
    "- **Class 1 ('three') has only 4 samples** - This is the main blocker\n",
    "- **81% of data is 'yes'** - Severe imbalance\n",
    "- **Metrics stuck** - Model not learning\n",
    "\n",
    "### Is 99.9% Realistic?\n",
    "\n",
    "**For your current dataset: NO** - Here's why:\n",
    "\n",
    "1. **Class 1 is too rare** (4 samples) - Cannot learn reliably\n",
    "2. **Class imbalance too extreme** (81% vs 0.5%)\n",
    "3. **Small dataset** (~750 training samples)\n",
    "\n",
    "### What You Can Achieve:\n",
    "\n",
    "| Scenario | Realistic Accuracy |\n",
    "|----------|-------------------|\n",
    "| **Remove Class 1** (binary: yes/no) | **95-98%** ✅ |\n",
    "| **Keep all 3 classes, balanced** | **85-92%** |\n",
    "| **Keep all 3 classes, current imbalance** | **80-85%** (current) |\n",
    "| **99.9% with current data** | **Not realistic** ❌ |\n",
    "\n",
    "### Recommended Path to High Accuracy:\n",
    "\n",
    "1. **Remove or merge Class 1** → Binary classification (yes/no)\n",
    "2. **Balance remaining classes** → Oversample 'no' to match 'yes'\n",
    "3. **Use optimized hyperparameters** → Higher LR, more epochs\n",
    "4. **Full fine-tuning** → Not LoRA (for maximum performance)\n",
    "5. **Ensemble models** → Train 3-5 models, average predictions\n",
    "\n",
    "### Expected Results After Fixes:\n",
    "\n",
    "- **Binary (yes/no)**: 95-98% accuracy ✅\n",
    "- **3-class balanced**: 85-92% accuracy\n",
    "- **99.9%**: Requires much more data or different approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eae808",
   "metadata": {},
   "source": [
    "## What Was Wrong With Your Training\n",
    "\n",
    "### Root Causes Identified:\n",
    "\n",
    "1. **Model Stuck Predicting \"yes\" Class**\n",
    "   - Diagnostic showed model predicts \"yes\" for ALL samples\n",
    "   - This means the model never learned to differentiate classes\n",
    "   - \"yes\" is likely the most frequent class (80%+ of data)\n",
    "\n",
    "2. **Learning Rate Too Low**\n",
    "   - Previous: `1e-5` was too conservative\n",
    "   - Model couldn't break out of initial bias\n",
    "   - **Fixed**: Increased to `3e-5` for faster learning\n",
    "\n",
    "3. **Class Weights Not Strong Enough**\n",
    "   - Previous formula: `total_samples / (num_labels * count)`\n",
    "   - Didn't give enough penalty to majority class\n",
    "   - **Fixed**: Using `sqrt(max_count / count)` for stronger weighting\n",
    "\n",
    "4. **Wrong Metric for Best Model**\n",
    "   - Previous: Used `eval_loss` (model optimized for frequent class)\n",
    "   - **Fixed**: Now uses `balanced_accuracy` (optimizes for all classes)\n",
    "\n",
    "5. **Metrics Not Changing**\n",
    "   - All metrics identical = model not learning\n",
    "   - Need stronger class weights + higher learning rate\n",
    "\n",
    "### Improvements Made:\n",
    "\n",
    "✅ **Stronger Class Weights**: `sqrt(max_count / count)` gives minority classes more weight  \n",
    "✅ **Higher Learning Rate**: `3e-5` (was `1e-5`) to break out of stuck state  \n",
    "✅ **Better Metric**: `balanced_accuracy` instead of `eval_loss`  \n",
    "✅ **More Epochs**: 8 epochs (was 6) to allow more learning  \n",
    "✅ **Device Safety**: Better handling of class weights on correct device  \n",
    "✅ **Prediction Tracking**: Logs prediction diversity during training  \n",
    "\n",
    "### What to Expect Now:\n",
    "\n",
    "- **Balanced Accuracy**: Should increase from 0.33 to 0.5+ (ideally 0.7+)\n",
    "- **Macro F1**: Should increase from 0.30 to 0.5+ (ideally 0.7+)\n",
    "- **Metrics Will Change**: No longer identical across epochs\n",
    "- **Prediction Diversity**: Model should predict different classes, not just \"yes\"\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Re-run Cell 4** (training config) - to get new learning rate\n",
    "2. **Re-run Cell 5.5** (weighted trainer) - to get stronger class weights\n",
    "3. **Re-run Cell 6** (training) - should see improvement now\n",
    "\n",
    "If still stuck after these changes:\n",
    "- Further increase learning rate to `5e-5`\n",
    "- Reduce dropout to `0.1` (may be too high at 0.25)\n",
    "- Check if class distribution is too extreme (one class >90% of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "260e2f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Evaluating Model Performance\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Results:\n",
      "  Loss: 0.4816\n",
      "  Accuracy: 0.8138 (81.38%)\n",
      "  F1 Score: 0.2991\n",
      "\n",
      "======================================================================\n",
      "Getting Detailed Predictions\n",
      "======================================================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.00      0.00      0.00        34\n",
      "       three       0.00      0.00      0.00         1\n",
      "         yes       0.81      1.00      0.90       153\n",
      "\n",
      "    accuracy                           0.81       188\n",
      "   macro avg       0.27      0.33      0.30       188\n",
      "weighted avg       0.66      0.81      0.73       188\n",
      "\n",
      "\n",
      "✓ Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluate Model Performance\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Evaluating Model Performance\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Set Results:\")\n",
    "print(f\"  Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"  Accuracy: {eval_results['eval_accuracy']:.4f} ({eval_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"  F1 Score: {eval_results['eval_f1']:.4f}\")\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Getting Detailed Predictions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    predicted_labels, \n",
    "    target_names=[str(cls) for cls in le.classes_[:20]],  # Show first 20 classes\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "print(\"\\n✓ Evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf0da7",
   "metadata": {},
   "source": [
    "## Summary: Improvements to Fix Low Accuracy\n",
    "\n",
    "### Key Problems Identified:\n",
    "\n",
    "1. **Model Stuck Predicting Same Class**: All metrics identical across epochs suggests model always predicts the most frequent class\n",
    "2. **Severe Class Imbalance**: Balanced accuracy (0.33) vs Accuracy (0.81) shows model ignores minority classes\n",
    "3. **No Class Weighting**: Loss function treats all classes equally, so model optimizes for frequent classes\n",
    "\n",
    "### Solutions Implemented:\n",
    "\n",
    "1. **✅ Class-Weighted Loss Function** (Cell 5.5)\n",
    "   - Created `WeightedTrainer` that uses inverse frequency weighting\n",
    "   - Minority classes get higher weights in loss calculation\n",
    "   - Forces model to learn from all classes, not just frequent ones\n",
    "\n",
    "2. **✅ Class Distribution Analysis** (Cell 5.5)\n",
    "   - Shows exact distribution of classes in training set\n",
    "   - Calculates appropriate weights for each class\n",
    "   - Helps identify if certain classes are too rare\n",
    "\n",
    "3. **✅ Diagnostic Tools** (Cell 6.5)\n",
    "   - Checks model predictions before training\n",
    "   - Identifies if model is stuck predicting same class\n",
    "   - Helps debug initialization issues\n",
    "\n",
    "### Expected Improvements:\n",
    "\n",
    "- **Balanced Accuracy**: Should increase from 0.33 to 0.5+ (ideally 0.7+)\n",
    "- **Macro F1**: Should increase from 0.30 to 0.5+ (ideally 0.7+)\n",
    "- **Accuracy**: May decrease slightly (from 0.81) but overall performance improves\n",
    "- **Metrics Should Change**: No longer identical across epochs\n",
    "\n",
    "### Additional Recommendations:\n",
    "\n",
    "1. **If still stuck**: \n",
    "   - Increase learning rate to 2e-5 or 3e-5\n",
    "   - Reduce dropout to 0.1 (may be too high)\n",
    "   - Check if class weights are too extreme\n",
    "\n",
    "2. **If balanced accuracy improves but still low**:\n",
    "   - Further reduce number of classes (top-K = 50 instead of 100)\n",
    "   - Increase min_count threshold\n",
    "   - Consider oversampling minority classes\n",
    "\n",
    "3. **Monitor training**:\n",
    "   - Watch for balanced_accuracy improving\n",
    "   - Check if predictions become more diverse\n",
    "   - Ensure loss is decreasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c9fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Creating Inference Function\n",
      "======================================================================\n",
      "\n",
      "Testing Inference Function:\n",
      "======================================================================\n",
      "\n",
      "Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
      "Predicted Answer: The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'confidence_pct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_answer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence_pct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 3 Predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'confidence_pct'"
     ]
    }
   ],
   "source": [
    "# 8. Inference Function - Predict Answers for New Questions\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Creating Inference Function\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def predict_answer(question, model, tokenizer, label_encoder, device='cpu', top_k=top_k):\n",
    "    \"\"\"\n",
    "    Predict answer for a given question\n",
    "    \n",
    "    Args:\n",
    "        question: Question string\n",
    "        model: Trained BERT model\n",
    "        tokenizer: BERT tokenizer\n",
    "        label_encoder: LabelEncoder used for encoding answers\n",
    "        device: Device to run inference on\n",
    "        top_k: Number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predicted answer and confidence scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize question\n",
    "    encoding = tokenizer(\n",
    "        question,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_probs, top_indices = torch.topk(probabilities[0], k=min(top_k, len(label_encoder.classes_)))\n",
    "    \n",
    "    # Decode predictions\n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        answer = label_encoder.inverse_transform([idx.item()])[0]\n",
    "        results.append({\n",
    "            'answer': answer,\n",
    "            'confidence': prob.item(),\n",
    "            'confidence_pct': f\"{prob.item()*100:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'predicted_answer': results[0]['answer'],\n",
    "        'confidence': results[0]['confidence'],\n",
    "        'top_predictions': results\n",
    "    }\n",
    "\n",
    "# Test the inference function\n",
    "print(\"\\nTesting Inference Function:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_questions = [\n",
    "    \"Was Abraham Lincoln the sixteenth President of the United States?\",\n",
    "    \"Did Lincoln sign the National Banking Act of 1863?\",\n",
    "    \"Did his mother die of pneumonia?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    result = predict_answer(question, model, tokenizer, le, device=device)\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Predicted Answer: {result['predicted_answer']}\")\n",
    "    print(f\"Confidence: {result['confidence_pct']}\")\n",
    "    if len(result['top_predictions']) > 1:\n",
    "        print(f\"Top 3 Predictions:\")\n",
    "        for i, pred in enumerate(result['top_predictions'][:3], 1):\n",
    "            print(f\"  {i}. {pred['answer']} ({pred['confidence_pct']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ Inference function ready!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464956be",
   "metadata": {},
   "source": [
    "## Summary: Super Fine-Tuning BERT for QA Tasks\n",
    "\n",
    "### What We Did:\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Loaded and cleaned question-answer pairs\n",
    "   - Encoded answers as labels for classification\n",
    "   - Split data into train/validation sets (80/20)\n",
    "\n",
    "2. **Dataset Creation**\n",
    "   - Created `QADataset` class compatible with HuggingFace Trainer\n",
    "   - Tokenized questions with BERT tokenizer\n",
    "   - Prepared data in format: `{input_ids, attention_mask, token_type_ids, labels}`\n",
    "\n",
    "3. **Model Setup**\n",
    "   - Loaded `bert-base-uncased` with classification head\n",
    "   - Configured for multi-class classification (number of answer classes)\n",
    "\n",
    "4. **Training Configuration**\n",
    "   - Learning rate: 2e-5 (conservative for fine-tuning)\n",
    "   - Weight decay: 0.05 (regularization)\n",
    "   - Batch size: 16 (training), 32 (evaluation)\n",
    "   - Early stopping: patience=2 epochs\n",
    "   - Mixed precision (FP16) if GPU available\n",
    "\n",
    "5. **Training & Evaluation**\n",
    "   - Trained with automatic evaluation after each epoch\n",
    "   - Saved best model based on validation loss\n",
    "   - Computed accuracy and F1 scores\n",
    "\n",
    "6. **Inference**\n",
    "   - Created function to predict answers for new questions\n",
    "   - Returns top-k predictions with confidence scores\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- ✅ **Super Fine-Tuning**: Conservative learning rate (2e-5) for stable training\n",
    "- ✅ **Early Stopping**: Prevents overfitting\n",
    "- ✅ **Best Model Saving**: Automatically saves best checkpoint\n",
    "- ✅ **Evaluation Metrics**: Accuracy and F1 score\n",
    "- ✅ **Inference Ready**: Easy-to-use prediction function\n",
    "\n",
    "### Next Steps to Improve:\n",
    "\n",
    "1. **If validation loss is high**:\n",
    "   - Reduce learning rate to 1e-5\n",
    "   - Increase weight decay to 0.1\n",
    "   - Freeze lower BERT layers (uncomment in Cell 4)\n",
    "\n",
    "2. **If overfitting**:\n",
    "   - Increase dropout in model config\n",
    "   - Use more aggressive early stopping (patience=1)\n",
    "   - Reduce number of epochs\n",
    "\n",
    "3. **If underfitting**:\n",
    "   - Increase learning rate to 3e-5\n",
    "   - Train for more epochs\n",
    "   - Reduce weight decay\n",
    "\n",
    "4. **For better performance**:\n",
    "   - Use larger batch size if GPU memory allows\n",
    "   - Try different learning rate schedules (cosine annealing)\n",
    "   - Fine-tune hyperparameters based on validation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "466532f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Comparison: Base Model vs Fine-Tuned Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Step 1: Loading Base BERT Model (No Fine-Tuning)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Base model loaded on device: cpu\n",
      "\n",
      "======================================================================\n",
      "Step 2: Evaluating Base Model\n",
      "======================================================================\n",
      "\n",
      "Base Model Results:\n",
      "  Loss: 1.2982\n",
      "  Accuracy: 0.0053 (0.53%)\n",
      "  Balanced Accuracy: 0.3333 (33.33%)\n",
      "  Micro F1: 0.0053\n",
      "  Weighted F1: 0.0001\n",
      "  Macro F1: 0.0035\n",
      "\n",
      "======================================================================\n",
      "Step 3: Getting Fine-Tuned Model Results\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-Tuned Model Results:\n",
      "  Loss: 0.9570\n",
      "  Accuracy: 0.8138 (81.38%)\n",
      "  Balanced Accuracy: 0.3333 (33.33%)\n",
      "  Micro F1: 0.8138\n",
      "  Weighted F1: 0.7303\n",
      "  Macro F1: 0.2991\n",
      "\n",
      "======================================================================\n",
      "Step 4: Side-by-Side Comparison\n",
      "======================================================================\n",
      "\n",
      "                Metric Base Model (No SFT) Fine-Tuned Model (SFT)\n",
      "Loss (Lower is Better)              1.2982                 0.9570\n",
      "              Accuracy      0.0053 (0.53%)        0.8138 (81.38%)\n",
      "     Balanced Accuracy     0.3333 (33.33%)        0.3333 (33.33%)\n",
      "              Micro F1              0.0053                 0.8138\n",
      "           Weighted F1              0.0001                 0.7303\n",
      "              Macro F1              0.0035                 0.2991\n",
      "\n",
      "======================================================================\n",
      "Improvement Analysis\n",
      "======================================================================\n",
      "\n",
      "Loss Improvement: +26.28% (✓ Better)\n",
      "Accuracy Improvement: +15200.00% (✓ Better)\n",
      "Balanced Accuracy Improvement: +0.00% (✗ Worse)\n",
      "Macro F1 Improvement: +8335.19% (✓ Better)\n",
      "\n",
      "======================================================================\n",
      "Step 5: Per-Class Performance Comparison\n",
      "======================================================================\n",
      "\n",
      "Base Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.00      0.00      0.00        34\n",
      "       three       0.01      1.00      0.01         1\n",
      "         yes       0.00      0.00      0.00       153\n",
      "\n",
      "    accuracy                           0.01       188\n",
      "   macro avg       0.00      0.33      0.00       188\n",
      "weighted avg       0.00      0.01      0.00       188\n",
      "\n",
      "\n",
      "Fine-Tuned Model Classification Report:\n",
      "(Using training metrics - detailed per-class report not available)\n",
      "\n",
      "======================================================================\n",
      "Step 6: Prediction Distribution Comparison\n",
      "======================================================================\n",
      "\n",
      "Base Model Prediction Distribution:\n",
      "  Class 0 ('no'): 1 predictions (0.5%)\n",
      "  Class 1 ('three'): 187 predictions (99.5%)\n",
      "\n",
      "======================================================================\n",
      "Comparison Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 10. Comparison: Base Model (No SFT) vs Fine-Tuned Model (SFT)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Comparison: Base Model vs Fine-Tuned Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load base model (no fine-tuning)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 1: Loading Base BERT Model (No Fine-Tuning)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "base_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "base_model = base_model.to(device)\n",
    "base_model.eval()\n",
    "\n",
    "print(f\"✓ Base model loaded on device: {device}\")\n",
    "\n",
    "# Step 2: Evaluate base model on validation set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 2: Evaluating Base Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "base_predictions = []\n",
    "base_true_labels = []\n",
    "base_logits_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_dataset)):\n",
    "        sample = val_dataset[i]\n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "        attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "        token_type_ids = sample['token_type_ids'].unsqueeze(0).to(device)\n",
    "        \n",
    "        outputs = base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits.cpu().numpy()[0]\n",
    "        pred = np.argmax(logits)\n",
    "        true_label = sample['labels'].item()\n",
    "        \n",
    "        base_predictions.append(pred)\n",
    "        base_true_labels.append(true_label)\n",
    "        base_logits_all.append(logits)\n",
    "\n",
    "base_predictions = np.array(base_predictions)\n",
    "base_true_labels = np.array(base_true_labels)\n",
    "base_logits_all = np.array(base_logits_all)\n",
    "\n",
    "# Calculate metrics for base model\n",
    "base_accuracy = accuracy_score(base_true_labels, base_predictions)\n",
    "base_balanced_accuracy = balanced_accuracy_score(base_true_labels, base_predictions)\n",
    "base_micro_f1 = f1_score(base_true_labels, base_predictions, average='micro', zero_division=0)\n",
    "base_weighted_f1 = f1_score(base_true_labels, base_predictions, average='weighted', zero_division=0)\n",
    "base_macro_f1 = f1_score(base_true_labels, base_predictions, average='macro', zero_division=0)\n",
    "\n",
    "# Calculate loss (cross-entropy)\n",
    "base_loss = torch.nn.functional.cross_entropy(\n",
    "    torch.tensor(base_logits_all),\n",
    "    torch.tensor(base_true_labels)\n",
    ").item()\n",
    "\n",
    "print(f\"\\nBase Model Results:\")\n",
    "print(f\"  Loss: {base_loss:.4f}\")\n",
    "print(f\"  Accuracy: {base_accuracy:.4f} ({base_accuracy*100:.2f}%)\")\n",
    "print(f\"  Balanced Accuracy: {base_balanced_accuracy:.4f} ({base_balanced_accuracy*100:.2f}%)\")\n",
    "print(f\"  Micro F1: {base_micro_f1:.4f}\")\n",
    "print(f\"  Weighted F1: {base_weighted_f1:.4f}\")\n",
    "print(f\"  Macro F1: {base_macro_f1:.4f}\")\n",
    "\n",
    "# Step 3: Get fine-tuned model results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 3: Getting Fine-Tuned Model Results\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use the trainer to evaluate fine-tuned model\n",
    "try:\n",
    "    if 'trainer' in globals():\n",
    "        sft_results = trainer.evaluate()\n",
    "        sft_loss = sft_results.get('eval_loss', 0)\n",
    "        sft_accuracy = sft_results.get('eval_accuracy', 0)\n",
    "        sft_balanced_accuracy = sft_results.get('eval_balanced_accuracy', 0)\n",
    "        sft_micro_f1 = sft_results.get('eval_micro_f1', 0)\n",
    "        sft_weighted_f1 = sft_results.get('eval_weighted_f1', 0)\n",
    "        sft_macro_f1 = sft_results.get('eval_macro_f1', 0)\n",
    "    elif 'trainer_optimized' in globals():\n",
    "        sft_results = trainer_optimized.evaluate()\n",
    "        sft_loss = sft_results.get('eval_loss', 0)\n",
    "        sft_accuracy = sft_results.get('eval_accuracy', 0)\n",
    "        sft_balanced_accuracy = sft_results.get('eval_balanced_accuracy', 0)\n",
    "        sft_micro_f1 = sft_results.get('eval_micro_f1', 0)\n",
    "        sft_weighted_f1 = sft_results.get('eval_weighted_f1', 0)\n",
    "        sft_macro_f1 = sft_results.get('eval_macro_f1', 0)\n",
    "    else:\n",
    "        # Evaluate fine-tuned model manually\n",
    "        model.eval()\n",
    "        sft_predictions = []\n",
    "        sft_true_labels = []\n",
    "        sft_logits_all = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(val_dataset)):\n",
    "                sample = val_dataset[i]\n",
    "                input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "                attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "                token_type_ids = sample['token_type_ids'].unsqueeze(0).to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits.cpu().numpy()[0]\n",
    "                pred = np.argmax(logits)\n",
    "                true_label = sample['labels'].item()\n",
    "                \n",
    "                sft_predictions.append(pred)\n",
    "                sft_true_labels.append(true_label)\n",
    "                sft_logits_all.append(logits)\n",
    "        \n",
    "        sft_predictions = np.array(sft_predictions)\n",
    "        sft_true_labels = np.array(sft_true_labels)\n",
    "        sft_logits_all = np.array(sft_logits_all)\n",
    "        \n",
    "        sft_loss = torch.nn.functional.cross_entropy(\n",
    "            torch.tensor(sft_logits_all),\n",
    "            torch.tensor(sft_true_labels)\n",
    "        ).item()\n",
    "        sft_accuracy = accuracy_score(sft_true_labels, sft_predictions)\n",
    "        sft_balanced_accuracy = balanced_accuracy_score(sft_true_labels, sft_predictions)\n",
    "        sft_micro_f1 = f1_score(sft_true_labels, sft_predictions, average='micro', zero_division=0)\n",
    "        sft_weighted_f1 = f1_score(sft_true_labels, sft_predictions, average='weighted', zero_division=0)\n",
    "        sft_macro_f1 = f1_score(sft_true_labels, sft_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nFine-Tuned Model Results:\")\n",
    "    print(f\"  Loss: {sft_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {sft_accuracy:.4f} ({sft_accuracy*100:.2f}%)\")\n",
    "    print(f\"  Balanced Accuracy: {sft_balanced_accuracy:.4f} ({sft_balanced_accuracy*100:.2f}%)\")\n",
    "    print(f\"  Micro F1: {sft_micro_f1:.4f}\")\n",
    "    print(f\"  Weighted F1: {sft_weighted_f1:.4f}\")\n",
    "    print(f\"  Macro F1: {sft_macro_f1:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error getting fine-tuned results: {e}\")\n",
    "    print(\"Using values from training history...\")\n",
    "    # Use the latest training metrics\n",
    "    sft_loss = 0.765116  # From epoch 4\n",
    "    sft_accuracy = 0.813830\n",
    "    sft_balanced_accuracy = 0.333333\n",
    "    sft_micro_f1 = 0.813830\n",
    "    sft_weighted_f1 = 0.730299\n",
    "    sft_macro_f1 = 0.299120\n",
    "\n",
    "# Step 4: Create comparison table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 4: Side-by-Side Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['Loss (Lower is Better)', 'Accuracy', 'Balanced Accuracy', 'Micro F1', 'Weighted F1', 'Macro F1'],\n",
    "    'Base Model (No SFT)': [\n",
    "        f\"{base_loss:.4f}\",\n",
    "        f\"{base_accuracy:.4f} ({base_accuracy*100:.2f}%)\",\n",
    "        f\"{base_balanced_accuracy:.4f} ({base_balanced_accuracy*100:.2f}%)\",\n",
    "        f\"{base_micro_f1:.4f}\",\n",
    "        f\"{base_weighted_f1:.4f}\",\n",
    "        f\"{base_macro_f1:.4f}\"\n",
    "    ],\n",
    "    'Fine-Tuned Model (SFT)': [\n",
    "        f\"{sft_loss:.4f}\",\n",
    "        f\"{sft_accuracy:.4f} ({sft_accuracy*100:.2f}%)\",\n",
    "        f\"{sft_balanced_accuracy:.4f} ({sft_balanced_accuracy*100:.2f}%)\",\n",
    "        f\"{sft_micro_f1:.4f}\",\n",
    "        f\"{sft_weighted_f1:.4f}\",\n",
    "        f\"{sft_macro_f1:.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Improvement Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "loss_improvement = ((base_loss - sft_loss) / base_loss) * 100 if base_loss > 0 else 0\n",
    "acc_improvement = ((sft_accuracy - base_accuracy) / base_accuracy) * 100 if base_accuracy > 0 else 0\n",
    "balanced_acc_improvement = ((sft_balanced_accuracy - base_balanced_accuracy) / base_balanced_accuracy) * 100 if base_balanced_accuracy > 0 else 0\n",
    "macro_f1_improvement = ((sft_macro_f1 - base_macro_f1) / base_macro_f1) * 100 if base_macro_f1 > 0 else 0\n",
    "\n",
    "print(f\"\\nLoss Improvement: {loss_improvement:+.2f}% ({'✓ Better' if loss_improvement > 0 else '✗ Worse'})\")\n",
    "print(f\"Accuracy Improvement: {acc_improvement:+.2f}% ({'✓ Better' if acc_improvement > 0 else '✗ Worse'})\")\n",
    "print(f\"Balanced Accuracy Improvement: {balanced_acc_improvement:+.2f}% ({'✓ Better' if balanced_acc_improvement > 0 else '✗ Worse'})\")\n",
    "print(f\"Macro F1 Improvement: {macro_f1_improvement:+.2f}% ({'✓ Better' if macro_f1_improvement > 0 else '✗ Worse'})\")\n",
    "\n",
    "# Step 5: Per-class comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 5: Per-Class Performance Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBase Model Classification Report:\")\n",
    "print(classification_report(base_true_labels, base_predictions, \n",
    "                          target_names=[str(cls) for cls in le.classes_], \n",
    "                          zero_division=0))\n",
    "\n",
    "print(\"\\nFine-Tuned Model Classification Report:\")\n",
    "if 'sft_predictions' in locals():\n",
    "    print(classification_report(sft_true_labels, sft_predictions, \n",
    "                              target_names=[str(cls) for cls in le.classes_], \n",
    "                              zero_division=0))\n",
    "else:\n",
    "    print(\"(Using training metrics - detailed per-class report not available)\")\n",
    "\n",
    "# Step 6: Prediction distribution comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 6: Prediction Distribution Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "base_pred_dist = Counter(base_predictions)\n",
    "sft_pred_dist = Counter(sft_predictions) if 'sft_predictions' in locals() else Counter()\n",
    "\n",
    "print(\"\\nBase Model Prediction Distribution:\")\n",
    "for label_id, count in sorted(base_pred_dist.items()):\n",
    "    class_name = le.inverse_transform([label_id])[0]\n",
    "    percentage = (count / len(base_predictions)) * 100\n",
    "    print(f\"  Class {label_id} ('{class_name}'): {count} predictions ({percentage:.1f}%)\")\n",
    "\n",
    "if sft_pred_dist:\n",
    "    print(\"\\nFine-Tuned Model Prediction Distribution:\")\n",
    "    for label_id, count in sorted(sft_pred_dist.items()):\n",
    "        class_name = le.inverse_transform([label_id])[0]\n",
    "        percentage = (count / len(sft_predictions)) * 100\n",
    "        print(f\"  Class {label_id} ('{class_name}'): {count} predictions ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Comparison Complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a72cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
